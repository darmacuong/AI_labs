{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DIT821 Software Engineering for AI systems\n",
    "\n",
    "DIT821 labs are derived from excercises from Coursera Machine Learning course. \n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "You are supposed to solve them yourself and submit the solutions. Further, you will be asked individually to explan the labs, in particuar the parts that you have written. The labs will be approved upon sucssesful correct submission and discussion. \n",
    "</div>\n",
    "\n",
    "\n",
    "* Name, e-mail:\n",
    "* Name, e-mail:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Exercise 9: Feature Engineering\n",
    "\n",
    "In this lab exercise, you will prepare your data in a way that it is suitable for a machine learning algorithm. You will achive this by first exploring the data and performing feature transformations on provided dataset of house price prediction ML problem. To do this exercise, you will need to have installed Python and required packages of external modules. Check installation guide document for more installation details:<br><br>\n",
    "\n",
    "- **scipy**: For advanced mathematical routines <br>\n",
    "- **numpy**: For N-dimensional array manipulation <br>\n",
    "- **pandas alias np**: For data analysis and data structures in DataFrames (i.e. tabular data with rows and columns)<br>\n",
    "- **matplotlib.pyplot alias plt**: For 2D plotting <br>\n",
    "- **sklearn**: For machine learning algorithms <br>\n",
    "- **Neptune.ai**: For experiment management(Optional)<br> \n",
    "\n",
    "\n",
    "## Files included in this exercise:\n",
    "\n",
    "- **Ames_Housing_dataset**\n",
    "    - train.csv \n",
    "    - data_description.txt \n",
    "\n",
    "## Where to get help:\n",
    "\n",
    "- **Working with Pandas**:\n",
    "    - Load data: <https://pandas.pydata.org/pandas-docs/stable/reference/io.html#flat-file>\n",
    "    - DataFrame: <https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html>\n",
    "    - Explore data: <https://pandas.pydata.org/pandas-docs/stable/reference/io.html#flat-file>\n",
    "    \n",
    "- **Working with Pandas and Matplotlib**: <https://pandas.pydata.org/pandas-docs/version/0.13/visualization.html>\n",
    "    - Visualize data e.g., scatterplot: <https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.scatter.html>\n",
    "    - Scatter Matrix: <https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.plotting.scatter_matrix.html>\n",
    "    -  Pivot table: <https://pandas.pydata.org/pandas-docs/stable/user_guide/reshaping.html> \n",
    "- **Working with Seaborn**: <https://seaborn.pydata.org/tutorial.html>  \n",
    "    - Heatmap: <https://seaborn.pydata.org/generated/seaborn.heatmap.html?highlight=heatmap#seaborn.heatmap>\n",
    "- **Working with sklearn**:\n",
    "    - train_test_split: <https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html><br>\n",
    "    - ML algorithms: <https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html>\n",
    "    - ML evaluation metrics: <https://scikit-learn.org/stable/modules/model_evaluation.html#mean-squared-error>\n",
    "- **(Optional) Working with Neptune.ai**:\n",
    "    - ML experiment management: <https://docs.neptune.ai/you-should-know/logging-metadata>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used for manipulating directory paths\n",
    "import os\n",
    "\n",
    "# For N-dimensional array manipulation\n",
    "import numpy as np\n",
    "\n",
    "# For advanced mathematical routines\n",
    "import scipy\n",
    "\n",
    "# Plotting library\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For data analysis and data structures in DataFrames \n",
    "import pandas as pd\n",
    "\n",
    "# For data visualization\n",
    "import seaborn as sns\n",
    "\n",
    "# For machine learning algorithms and evaluation metrics\n",
    "import sklearn\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "# tells matplotlib to embed plots within the notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Submission\n",
    "For this programming exercise you are required to train a ML model for predicting house prices using Ames Housing Dataset. The following is a breakdown of how each part of this exercise.  \n",
    "- *Step 1: Load dataset*<br>\n",
    "- *Step 2: Visualize and explore dataset in order to gain insights*<br>\n",
    "- *Step 3: Prepare data for ML (data cleaning and feature engineering)*<br>\n",
    "- *Step 4: Train and evaluate a simple linear regression model*<br>\n",
    "- *Step 5: Refine model using regularization*<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# House Price Prediction\n",
    "\n",
    "The ML problem is that of predicting house prices based on the provided Ames housing dataset. <br>\n",
    "\n",
    "## 1 Load Dataset\n",
    "In this first task, you will load dataset `train.csv` to a Panda's DataFrame object named `train_house_data`. You will then confirm that the data is loaded by looking at data structure. \n",
    "\n",
    "> **TASK 1.** Write code to load data using Pandas `read_csv()`, which will return a [DataFrame](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html) that you can explore. To briefly look at data structure, check and print:\n",
    "\n",
    "> - Data size with `DataFrame.shape` \n",
    "\n",
    "> - Data summary with `DataFrame.info()`\n",
    "\n",
    "> - First five rows with `DataFrame.head()`\n",
    "\n",
    "> - Summary of numeric features values (mean, min, max, std etc) with `DataFrame.describe()`\n",
    "\n",
    "> - Values of a categorical feature (Street) with `DataFrame['column'].value_counts()` \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1 Load dataset \n",
    "\n",
    "## Implement a function to load dataset\n",
    "def loadDataset(dirname, filename):\n",
    "     \"\"\"\n",
    "     Loads dataset from a csv file\n",
    "    \n",
    "     Parameters\n",
    "     ----------\n",
    "     dirname: String\n",
    "        The path to the directory containing the dataset.\n",
    "        \n",
    "     filename: String\n",
    "        The name of the file containing dataset .\n",
    "    \n",
    "     Returns\n",
    "     -------\n",
    "     df : Object\n",
    "        A pandas dataframe.\n",
    "    \n",
    "     \"\"\"\n",
    "    # ====================== YOUR CODE HERE ======================\n",
    "    \n",
    "    \n",
    "    \n",
    "    # =================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1 load dataset\n",
    "\n",
    "## Create a Panda's dataframe called 'house_data_df' by executing the load dataset function\n",
    "house_data_df = loadDataset('Data/','train.csv') # modify this if necessary\n",
    "\n",
    "## Brielfy look at the data using 'shape', 'head', 'info' etc, functions. \n",
    "## Comment out each function after implementation except for shape\n",
    "# ====================== YOUR CODE HERE ======================\n",
    "# Print DataFrame shape \n",
    "\n",
    "# Print DataFrame info\n",
    "\n",
    "# Print DataFrame head with 10 rows\n",
    "\n",
    "# Print DataFrame describe \n",
    "\n",
    "# Print the count of values in 'Street' using value_counts() \n",
    "\n",
    "# ==================================================================\n",
    "\n",
    "\n",
    "# Drop the 'Id' column \n",
    "# ====================== YOUR CODE HERE ======================\n",
    "\n",
    "\n",
    "# ==================================================================\n",
    "\n",
    "\n",
    "# print the new dataframe shape\n",
    "print('Shape after dropping ID: \\n', house_data_df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 1460 instances in the train dataset. Notice that some features have missing values that need to be take care of if you are to use the feature in your model. For example, 'Alley' feature has only 91 non-null values, meaning that 1369 instances are missing this value. <br><br>\n",
    "You can also already see that 37 features are numeric (38 if Id is not removed) and 43 of type object, which means they can hold any type of Python object. However since you loaded the data from a CSV file you can speculate they are text features. Also, when you look at the top five rows, you can notice that feature values of type object (e.g., MSZoning and Street) are repetitive, which means they are probably categorical features. \n",
    "You can find how many categories exist by using the Panda's `value_counts()` method. You can use Panda's `describe()` to show descriptive statitstics summary of the numerical features, such as mean. Notice that the `describe()` method excludes NaN values e.g, 'MasVnrArea' feature. See 1.2 for help\n",
    "\n",
    "\n",
    "## 2 Discover and Visualize Dataset to Gain Insights  \n",
    "\n",
    "So far you have briely looked at the data to get a general understanding of the kind of data you are manupulating. This is also a good way to inspect your data to find abnormalities and peculiarities. To help you achieve in-depth view of data, you will be doing some plotting as you explore the data. There are variety of ways to visualize data and gain insights, such as Histograms and Scatterplots. `Matplotlib.pyplot` module will be used for visualizations.\n",
    "\n",
    "> **TASK 2.** Write code to visualize relationships of features (remember to give names to title, xlable and ylable):\n",
    "\n",
    "> - Histogram of DataFrame using `DataFrame.hist()`\n",
    "\n",
    "> - Correlations of the target feature'SalePrice' with others features using `DataFrame.corr()` \n",
    "\n",
    "> - Scatterplot to visualize correlations of few selected features ('SalePrice', 'OverallQual','GrLivArea', 'GarageArea', 'TotalBsmtSF', 'LotArea') using `DataFrame.scatter_matrix()` \n",
    "\n",
    "> - Scatterplot of 'SalePrice' and ' GrLivArea' using `plt.scatter()`\n",
    "\n",
    "> - Pivot table using `DataFrame.pivot_table()` between target feature 'SalePrice' and categorical features (a)SaleCondition (b) Neighborhood and (c) MSZoning \n",
    "\n",
    "> Make sure you are able to inteprete the results and can give a brief explanation of the visualizations\n",
    "\n",
    "### Overview  with Histograms\n",
    "\n",
    "You can plot a histogram of each numerical feature to show number of instances for a given value range. `DataFrame.hist()` plots histogram over whole dataset, however in other cases you can choose to plot histogram of individual numerical feature.\n",
    "\n",
    "Based on these histograms, take note that features have very different scales (x-axis) and that some features have 'tail-heavy' skewed ditribution. You will now proceed to explore relationship between features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Task 2 Visualize data to gain insights (Histograms)\n",
    "\n",
    "# Plot histogram of numerical features using DataFrame.hist(figsize=(30,20)). \n",
    "# Notice, in the later, DataFrame is the created house_data_df\n",
    "#  ===== YOUR CODE HERE ==========\n",
    "\n",
    "\n",
    "\n",
    "# =================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking for Correlations\n",
    "\n",
    "Since the dataset is not too large, you can compute correlation coefficient between every pair of features using Panda's `corr()` method. Correlations coefficients measures linear correlations and ranges from -1 to 1, whereby values close to 1 mean strong positive correlation and correlations close to zero mean there is no linear correlation. \n",
    "\n",
    "Using Panda's `corr()`, you need to examine the correlation between the features and the target 'SalePrice'. You will notice that the target 'SalePrice'  variable is highly correlated with features like OverallQual, GrLivArea, GarageCars, GarageArea and TotalBsmtSF among others. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Task 2 Visualize data to gain insights  (Correlations)\n",
    "\n",
    "# Check correlation of 'SalePrice' with others features using DataFrame.corr()\n",
    "# Start by assigning the corrections (DataFrame.corr()) to a new object 'feature_corr'\n",
    "#  ===== YOUR CODE HERE ==========\n",
    "\n",
    "\n",
    "# Then, print correlation of 'Sale Price' with other features from 'feature_corr' and sort  in descending order\n",
    "#  ===== YOUR CODE HERE ==========\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Seaborn library, generate a heatmap based on 'feature_corr' from the previous cell\n",
    "#  ===== YOUR CODE HERE ==========\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview with Scatterplots\n",
    "\n",
    "You can use scatterplot to further explore data by checking the correlation between numeric features. Pandas' `pd.plotting.scatter_matrix()` plots every numerical feature against every other numerical feature. Since the dataset has several features you will only explore few promising features (with high correlation) namely: SalePrice, OverallQual, GrLivArea, GarageArea, TotalBsmtSF, and LotArea\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Task 2 Visualize data to gain insights (Scatterplots)\n",
    "\n",
    "# Plot scatterplots to visualize correlations of few selected features \n",
    "# Start by assigning the specified features to list called 'scatter_features'. \n",
    "#  ===== YOUR CODE HERE ==========\n",
    "\n",
    "\n",
    "\n",
    "# Then, plot the scatter matrix using pd.plotting.scatter_matrix() function\n",
    "# Appropriately pass the parameters that utilizes above 'scatter features' to the function\n",
    "#  ===== YOUR CODE HERE ==========\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the scatterplot you can confirm strong positive correlations with the upward trends and notice that 'OverallQual' is a categorial feature. Among the most promising feature to predict 'SalePrice' is GrLivArea. Use `plt.scatter()` to further get detailed view of the correlations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Task 2 Visualize data to gain insights (scatterplot-detailed)\n",
    "\n",
    "# Plot a scatterplot of SalesPrice and GrLivArea\n",
    "# ===== YOUR CODE HERE ==========\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on correlation results, you have used scatterplot to further explore'GrLivArea' and 'SalePrice' features that revealed positive correlation through the upward trend. At the same time, outliers can be identified that show relatively low prices for large house sizes. Outliers are observed in houses with over 4000 GrLivArea as they contain either extremely high prices or low prices for such big area sizes.<br>\n",
    "\n",
    "These outliers need to be removed to prevent your ML algorithm from learning from them. Therefore, you will clean the data by removing outliers and only train on houses with less than 4000 GrLvArea. (Notice that often you will need to assess the impact of removing outliers prior to deciding their removal)\n",
    "\n",
    "*Notice:* `DataFrame.loc[]` allows you to access a group of rows and columns by label(s).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing houses with GrLivArea of more than 4000, as these contain outliers\n",
    "df_outlier = house_data_df.loc[house_data_df['GrLivArea'] > 4000]\n",
    "house_data_df = house_data_df.drop(df_outlier.index)\n",
    "print(house_data_df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relationships with Pivot Table\n",
    "\n",
    "A pivot table is a table of statistics that summarizes the data of a more extensive table. In our data, a categorical feature 'OverallQual' has unique values in the interval of 1 to 10 inclusive.<br><br>\n",
    "You can create a pivot table to investigate the relationship between this categorical feature (e.g., OverallQual) and SalePrice. This is accomplished with Panda's `pivot_table()` by setting index='OverallQual'and values='SalePrice' and choosing to look at the median. Use `Series.plot()` to visualize the pivot table more easily by creating the bar plot. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Task 2 Visualize data to gain insights (Pivot table)\n",
    "\n",
    "# Plot pivot tables using DataFrame.pivot_table() \n",
    "\n",
    "# Pivot table to investigate relationship of OveralQual and SalePrice\n",
    "quality_pivot = house_data_df.pivot_table(index='OverallQual', values='SalePrice', aggfunc=np.median)\n",
    "quality_pivot.plot(kind='bar',label='Overall Quality',color='blue')\n",
    "\n",
    "# Pivot table to evaluate relationship of SaleCondition and SalePrice\n",
    "# ===== YOUR CODE HERE ==========\n",
    "\n",
    "\n",
    "# Pivot table to evaluate relationship of Neighborhood and SalePrice\n",
    "# ===== YOUR CODE HERE ==========\n",
    "\n",
    "\n",
    "# Pivot table to evaluate relationship of MSZoning and SalePrice\n",
    "# ===== YOUR CODE HERE ==========\n",
    "\n",
    "\n",
    "plt.ylabel('Median Sale Price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 3 Prepare Dataset for Machine Learning \n",
    "\n",
    "The exploration stage is an iterative process. In the beginning, the goal is to gain some first insights that is sufficiently for developing a first (ideally, reasonably good) prototype and then refine the exploration in the next iterations. \n",
    "\n",
    "The previous tasks have given you some insights into the dataset, which you may use to prepare for ML algorithm. You were able to identify and remove some outliers and to see some interesting correlations between the target and the other features. Furthermore, histograms showed tail-heavy distribution 'skew' requiring transformations, and transformations of non-numeric features have so far not been explored. You will now write code to prepare dataset for ML algorithms.\n",
    "\n",
    "\n",
    "> **TASK 3:** Overview:\n",
    "\n",
    "> - Clean and engineer numeric features (handle missing values and perform log transformation). See Task 3.1 for more details\n",
    "\n",
    "> - Clean and engineer categorical features (handle missing values, perform one-hot encoding). See Task 3.2 for more details\n",
    "\n",
    "### Separating features and target variable\n",
    "\n",
    "First, since we do not necessarily need to apply the same transformations to predictors and target feature, we separate out the target feature. We will use Panda's `drop('column', axis =1)`,  which creates a copy of the DataFrame and drops the specified column without affecting the original DataFrame.\n",
    "\n",
    "We can separate features and the target variable using `DataFrame.drop('column', axis =1)` for features DataFrame object and `DataFrame['column'].copy()` for target DataFrame object. We will use  `house_data_features` and `house_data_target` as names of the new DataFrame objects.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating features and target from house dataset \n",
    "house_data_features = house_data_df.drop(columns='SalePrice', axis =1) \n",
    "house_data_target = house_data_df[\"SalePrice\"].copy() \n",
    "house_data_target.columns = ['SalePrice']\n",
    "\n",
    "# Print DataFrame shape of features and target variable\n",
    "print('Shape of features: \\n', house_data_features.shape)\n",
    "print('Shape of target: \\n', house_data_target.shape)\n",
    "house_data_features.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning - *Handling missing values*\n",
    "\n",
    "Missing values in our data table are blank spaces and placeholder strings, e.g., ''Not A Number (NaN)''. Most computational tools are unable to handle such missing values or would produce unpredictable results if we simply ignored them. Therefore, it is crucial that we take care of the missing values before we proceed with further analyses. \n",
    "\n",
    "In this exercise, you can accomplish handling of missing values by using Panda's `DataFrame.dropna()`, `DataFrame.drop()` and `DataFrame.dropfillna()`. To get an overview of missing values in current train dataset you will use the `isnull()` to return a DataFrame with Boolean values that indicate whether data is missing. The `sum()` can further be used to return the number of missing values per column. \n",
    "\n",
    "You typically have three options to deal with missing values:\n",
    "\n",
    "- Get rid of the instances with missing values with Panda's `dropna()`\n",
    "\n",
    "- Get rid of the whole feature with Panda's `drop()`\n",
    "\n",
    "- Set the missing values to some value(zeros, mean, median, etc) with Panda's `fillna()`\n",
    "\n",
    "Notice the following: In real-world deployment contexts,  you cannot be sure there will not be any missing values in new data after training and deploying a ML model. Therefore, you need to apply handling of missing value not just to the current missing values. However, in this exercise we only handle missing values in existing data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check for missing values\n",
    "def checkMissing(df):\n",
    "    \"\"\"\n",
    "    Checks missing value in a panda's dataframe and returns count\n",
    "    \n",
    "    Parameters\n",
    "     ----------\n",
    "    df: Object\n",
    "        A panda's DataFrame to check for missing values\n",
    "        \n",
    "    Returns\n",
    "     ----------\n",
    "    missing: Object\n",
    "            A panda's object with count of missing values\n",
    "            \n",
    "    \"\"\"\n",
    "    missing_count = df.isna().sum().sort_values(ascending= False)\n",
    "    missing_count = missing_count[missing_count != 0]\n",
    "    return missing_count\n",
    "    \n",
    "\n",
    "# Checking for missing values in features\n",
    "print(checkMissing(house_data_features))\n",
    "\n",
    "# Print DataFrame shape of features and target variable\n",
    "print('Shape of features (1456, 79): \\n', house_data_features.shape)\n",
    "print('Shape of target (1456,): \\n', house_data_target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Numeric Features (Data Cleaning and Feature Engineering)\n",
    "\n",
    "As you start to consider features, you can check numeric features using Panda’s `DataFrame.select_dtypes()` to return subset of columns matching the specified data types. To specify numeric datatype pass `include=[np.number]` which uses numpy datatype function.\n",
    "\n",
    "You will process numeric features by first handling the missing values, and then handle skewness with for example log-transformation or perfom feature rescaling.\n",
    "\n",
    "\n",
    ">**TASK 3.1** Write code to:\n",
    "\n",
    "> - Create a DataFrame object from `house_data_features` DataFrame object to contain numeric features `numeric_features`. You can use  `DataFrame.select_dtypes()` and passing `include=[np.number]` to get numeric features. \n",
    "\n",
    "> - Check and handle missing values of numeric features. Check missing values in dataset using `DataFrame.isnull().sum().sort_values(ascending=False)`. See 3.1.1 handling missing values of numeric features for implementation details\n",
    "\n",
    "> - Engineer the numeric features. See 3.1.2 Processing of numeric features for implementation details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Task 3.1 Numeric features (Data cleaning - Handling missing values)\n",
    "\n",
    "# Create a new object 'numeric_features' from house_data_features that contains only numeric features\n",
    "# Print the shape of 'numeric features' dataframe object. Expected shape is (1456, 36)\n",
    "# ===== YOUR CODE HERE ==========\n",
    "\n",
    "\n",
    "\n",
    "# Print numeric_features with missing values using the 'checkMissing()' function created above \n",
    "# ===== YOUR CODE HERE ==========\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 Handling missing values of numeric features\n",
    "\n",
    "The dataset contains missing values for 'LotFrontage', 'GarageYrBlt' and 'MasVnrArea. You will proceed to handle the missing values in the following ways:\n",
    "\n",
    "- LotFrontage: fill median value (sort data by ‘Neighborhood’ before imputing median). Use Panda's `DataFrame.groupby()` and `DataFrame.transform()`\n",
    "- Otherwise fill missing values with 0 using `fillna()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Task 3.1.1 Handling missing values of numeric features\n",
    "\n",
    "# Fill nulls for 'LotFrontage' with median value calculated after grouping by 'Neighborhood'\n",
    "# ===== YOUR CODE HERE ==========\n",
    "\n",
    "\n",
    "\n",
    "# Fill nulls for 'GarageYrBlt','MasVnrArea' with 0\n",
    "# ===== YOUR CODE HERE ==========\n",
    "\n",
    "\n",
    "\n",
    "# Check that features LotFrontage, GarageYrBl and MasVnrArea are no longer in missing value list \n",
    "print(checkMissing(house_data_features))\n",
    "\n",
    "# Print DataFrame shape of features and target variable\n",
    "print('Shape of features (1456, 79): \\n', house_data_features.shape)\n",
    "print('Shape of target (1456,): \\n', house_data_target.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 Feature engineering of numeric features (log-transform)\n",
    "\n",
    "When performing regression, sometimes it is good to apply log-transformation to the target variable, in particular, when it is skewed. Importantly,  the predictions generated by the final model  will also be log-transformed; so, you will need to convert the predictions back to the original form later.\n",
    "\n",
    "You will use numpy's log-transformation function `np.log()` and pass DataFrame object to apply the transformation. You will apply log-transformation to target 'SalePrice' variable. You can use `np.exp()` to reverse the transformation. Check for skeweness before and after log-transformation using Panda's `skew()` function specifically DataFrame.SalePrice.skew()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Task 3.1.2 Feature Engineering for Numeric features (log-transformation)\n",
    "\n",
    "# Check skewness of the target variable\n",
    "print(\"Skew (no log transform): 1.565959 \\n\", house_data_target.skew())\n",
    "\n",
    "# Apply log-transform on target feature 'SalePrice' using 'np.log()' function (Execute only once)\n",
    "# ===== YOUR CODE HERE ==========\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Check skewness of the target variable after log-transform\n",
    "print(\"Skew (log transform): 0.065448 \\n\", house_data_target.skew())\n",
    "\n",
    "# Print DataFrame shape of features and target variable\n",
    "print('Shape of features (1456, 79): \\n', house_data_features.shape)\n",
    "print('Shape of target (1456,): \\n', house_data_target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Categorical Features (Feature Engineering)\n",
    "\n",
    "Since many machine learning algorithms prefer to work with numbers, you will covert categories of categorical features from text to numbers. To do so, you will use Pandas one-hot encoding `get_dummies()`. You will first need to handle missing data of categorical features. Often, it is better to gather domain knowledge in order to make the best decision when dealing with missing data of categorical features. In our exercise, the Ames Housing documentation can help in understanding the missing values. For example, *PoolQC* features the value is NaN when PoolArea is 0, meaning  there is no Pool.\n",
    "\n",
    "\n",
    "> **TASK 3.2** Write code to:\n",
    "\n",
    "> - Handle numeric features that were deemed categorical by changing their dttype to object using `DataFrame[col].apply(str)`\n",
    "\n",
    "> - Create a DataFrame object from `house_data_features` DataFrame object to contain categorical features `categorical_features`. Use  `DataFrame.select_dtypes()` and passing `exclude=[np.number]` to get categorical features. Similar to first part of Task 3.1\n",
    "\n",
    "> - Check and handle missing values of categorical features.  Check missing values in dataset using `DataFrame.isnull().sum().sort_values(ascending=False)`. Handle missing values for categorical features (PoolQC,MiscFeature, Alley, Fence, FireplaceQu, GarageCond, GarageType, GarageFinish, GarageQual,BsmtExposure, BsmtFinType2, BsmtCond, BsmtQual, MasVnrType) by filling 'None'. You remove the one observation with missing value for Electrical feature.\n",
    "\n",
    "> - Perform binning and one-hot encoding for categorical features. See 3.2.2. Processing of categorical features (binning and one-hot encoding) for implementation details.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Task 3.2 Feature Engineering for Categorical features (Handling missing values)\n",
    "\n",
    "# Transform some numerical features that are actually categorical\n",
    "for col in ['OverallQual','MSSubClass','OverallCond']:\n",
    "    house_data_features[col].apply(str)\n",
    "\n",
    "# Create a new object 'categorical_features' from house_data_features that contains only categorical features\n",
    "# ===== YOUR CODE HERE ==========\n",
    "\n",
    "\n",
    "\n",
    "# Print missing values in categorical features using the checkMissing() function\n",
    "# ===== YOUR CODE HERE ==========\n",
    "\n",
    "\n",
    "\n",
    "print(\"Numeric features shape (1456, 36): \\n\", numeric_features.shape)\n",
    "print(\"Categorical features shape (1456, 43): \\n\", categorical_features.shape)\n",
    "print(\"House data features shape (1456,79): \\n\", house_data_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Handling missing values of categorical features \n",
    "\n",
    "Several categorical features have missing value. You will handle values of text and categorical as follows as noted in the data description:\n",
    "\n",
    "- PoolQC: NA means 'no pool', thus fill \"None\"\n",
    "\n",
    "- MiscFeature: NA means 'no misc feature', thus fill \"None\"\n",
    "\n",
    "- Alley: NA mean 'no alley access', thus fill \"None\"\n",
    "\n",
    "- Fence: NA means 'no fence', thus fill \"None\"\n",
    "\n",
    "- FireplaceQu: NA means 'no fireplace', thus fill \"None\"\n",
    "\n",
    "- GarageCond, GarageType, GarageFinish and GarageQual: fill \"None\"\n",
    "\n",
    "- BsmtExposure, BsmtFinType2, BsmtCond, BsmtQual: fill \"None\", since NaN means no basement\n",
    "\n",
    "- Electrical: Only one observation with NAN, you can fill it with 'SBrkr'(highest mode value) or remove observation with missing value, using `dropna(subset=['column'])`. If you choose to remove the observation you need to do it before separating features and target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Task 3.2 Feature Engineering for Categorical features (Handling missing values)\n",
    "\n",
    "# Fill nulls for the missing values in the specified categorical features of the \"house_data_features\" \n",
    "# according to the data description above (3.2.1).\n",
    "# Note the special description for feature 'Electrical'.\n",
    "# ===== YOUR CODE HERE ==========\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Check that there are no nulls in the dataframe \n",
    "print(checkMissing(house_data_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Processing of categorical features (binning and one-hot encoding)\n",
    "For categorical features, you may want to use **binning** and **one-hot encoding** to make transformations useful for building the model. You can do this by using Panda's `DataFrame.replace()` and `DataFrame.get_dummies()` respectively.\n",
    "\n",
    "When using one-hot encoding, it is important to remember that for a number of categories of a categorical feature you will create a set of new features equal to the number of its categories. In order not to increase the number of new features, you will perform binning on selected categorical features. Since some categorical features contain information in their ordering set we can use binning to preserved their ordering based on value range. Use binning and convert values of 'OverallQual and OverallCond' features into the following bins: Poor(1-2), Fair(3), Average (4-5), Good(6-7) and Excellent (8-10).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Task 3.2 Feature Engineering for Categorical features (one-hot encoding)\n",
    "\n",
    "# Print the shape of dataset before one-hot encoding\n",
    "print('Shape of features before one-hot encoding (1456,79):\\n', house_data_features.shape)\n",
    "\n",
    "\n",
    "# Perform binning and convert values of 'OverallQual and OverallCond' features\n",
    "# ===== YOUR CODE HERE ==========\n",
    "\n",
    "\n",
    "\n",
    "# Perform one-hot encoding on all categorical features\n",
    "# ===== YOUR CODE HERE ==========\n",
    "\n",
    "\n",
    "\n",
    "# Print DataFrame shape of features and target variable\n",
    "print('Shape of features after one-hot encoding:\\n', house_data_features.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Train and Evaluate a simple linear model\n",
    "\n",
    "In this part of the exercise you will build a simple linear regression model using `sklearn`. First use `train_test_split()` from sklearn to create training set and testing set. The train_test_split() returns four objects:\n",
    "- X_train, a subset of features used for training\n",
    "- X_test, a subset of features which will be test set\n",
    "- y_train, the target variable that corresponds to X_train\n",
    "- y_test, the target variable that corresponds to X_test\n",
    "\n",
    "Next, you will sent random_state=0 that provides reproduciable results. The test_size parameter tells the function what proportion of the data should be in the test partition. In this exercise, about 20% of the data is devoted to the test set.\n",
    "\n",
    "To build a simple linear regression model, you will first instantiate a linear regression model object `lr =  LinearRegression()` and then use `lr.fit()` to fit the model on training dataset to learn the weights. You will fit the model using X_train and y_train and then score/evaluate them using X_test and y_test. The weight parameters are stored in `coef_` attribute while the offset or intercept (b) is stored in the `intercept_` attribute.\n",
    "\n",
    "> **Task 4** Write code to:\n",
    "> - Create training set and testing set using train_test_split()` from sklearn.model_selection`\n",
    "\n",
    "> - Train a simple linear regression model from `sklearn.linear_model`\n",
    "\n",
    "> - Evaluate and print the results of your models with `MSE` or `R^2 score` from `sklearn metrics`\n",
    "\n",
    "> - (Optional) Track model score using Neptune.al\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "## Task 4 Train and Evaluate a simple linear regression model\n",
    "\n",
    "#  Split dataset in training set (X_train, y_train) and test set (X_test, y_test)\n",
    "# ===== YOUR CODE HERE ==========\n",
    "\n",
    "\n",
    "\n",
    "# Train a simple linear regression model 'lr' with training set\n",
    "# ===== YOUR CODE HERE ==========\n",
    "\n",
    "\n",
    "\n",
    "# Third, print model weights(coefficients) and intercept\n",
    "print(\"Model intercept (lr.intercept): \\n \", lr.intercept_)\n",
    "print(\"Model weights (lr.coef_) : \\n \", lr.coef_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(Optional Task)**\n",
    "\n",
    "In this optional part of the Lab, you will track some of the model metrics with Neptune.ai. You are required to setup Netptune.ai following the steps below. \n",
    "* You need to register a new account from https://neptune.ai/. \n",
    "* Install neptune-client library \n",
    "* Follow the setup wizard on Netptune.ai to create your first project. \n",
    "* Replace the credentials (i.e., project name and api_token) in the next cell with your own credential.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neptune.new as neptune\n",
    "\n",
    "# (Optional) Setup Neptune\n",
    "run = neptune.init(\n",
    "    project=\"userid/projectname\",# modify\n",
    "    api_token=\"\", # modify\n",
    ")  # your credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Task 4 Train and Evaluate a simple linear regression mode\n",
    "\n",
    "# Use the trained simple model to make predictions 'pred_test' on test set 'X_test' using the trained model \n",
    "# ===== YOUR CODE HERE ==========\n",
    "pred_test =  # modify line\n",
    "\n",
    "# Evaluate model performance using R^2 and mean squared error on training set i.e., pred_train, y_train\n",
    "# Remember to first make prediction pred_train for training set (X_train) using your simple linear model\n",
    "# ===== YOUR CODE HERE ==========\n",
    "pred_train =  # modify line\n",
    "R_square_train = # modify line\n",
    "RSME_train = # modify line\n",
    "\n",
    "print(\"R^2 score on trainig set: \", R_square_train)\n",
    "print(\"MSE score on trainig set: \", RSME_train)\n",
    "\n",
    "\n",
    "# Evaluate model performance using R^2 and mean squared error on test set i.e., pred_test, y_test\n",
    "# ===== YOUR CODE HERE ==========\n",
    "R_square_test = # modify line\n",
    "RSME_test = # modify line\n",
    "\n",
    "print(\"R^2 score on test set:\", R_square_test)\n",
    "print(\"MSE score on trainig set: \", RSME_test)\n",
    "\n",
    "\n",
    "# Plot the linear model\n",
    "plt.scatter(pred_test, y_test, color='red', marker='.', label = \"validation set\")\n",
    "plt.scatter(pred_train, y_train, color='blue', marker='.', label = \"Training set\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# (Optional) Using Neptune, track the metrics r_square and RMSE as metadata for both train and test.\n",
    "# ===== YOUR CODE HERE ==========\n",
    "\n",
    "\n",
    "\n",
    "# ================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interprete the results of `R^2`. The results of the simple linear model may not be ideal and discrepancy between performance on training set and test set is a sign of overfitting. You can work to further improve the model and control complexity by further engineering the features (e.g., by training with few informative features) or employing regularization (e.g., ridge and lasso models). \n",
    "\n",
    "# 5. Model refinement with regularization (Optional)\n",
    "\n",
    "One approach to restricting model overfitting is through regularization, which we have already considered in previous assignments. The following tasks are optional (ungraded), but a useful further exercise. \n",
    "\n",
    "## Ridge regression\n",
    "In Ridge regression, the weights (coefficients) are chosen not only so that they predict well but also their magnituded to be as small as possible. Intuitively this means that each feature should have little effect on the outcome as possible (i.e., little have slope) while predicting well. Ridge regression is implemented in `linear_model.Ridge`. See how well ridge regression does on the problem.\n",
    "\n",
    "> **Task 5.1** Write code to:\n",
    "> - Train a ridge regression model `sklearn.linear_model.Ridge`\n",
    "> - Evaluate and print the results of your models with `R^2 score` from `sklearn metrics.r2_score()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Task 5 Model refinement with regularization (Ridge regression)\n",
    "\n",
    "## Import required libraries\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge = Ridge()\n",
    "\n",
    "# Train a ridge regression model 'ridge' with training set\n",
    "# ===== YOUR CODE HERE ==========\n",
    "\n",
    "\n",
    "# Make predictions 'ridge_pred_train' on train set 'X_train' using the trained model and calculate the R^2\n",
    "# ===== YOUR CODE HERE ==========\n",
    "ridge_pred_train = 0 # modify\n",
    "R_square_train_ridge = 0 # modify\n",
    "\n",
    "print(\"R^2 score on trainig set: \", R_square_train)\n",
    "\n",
    "\n",
    "# Make predictions 'ridge_pred_test' on test set 'X_test' using the trained model and calculate the R^2\n",
    "# ===== YOUR CODE HERE ==========\n",
    "ridge_pred_test = 0 # modify\n",
    "R_square_test_ridge = 0 # modify\n",
    "\n",
    "print(\"R^2 score on trainig set: \", R_square_train)\n",
    "\n",
    "\n",
    "# (Optional) Using Neptune, track the R_square value.\n",
    "# ===== YOUR CODE HERE ==========\n",
    "\n",
    "\n",
    "# =================================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you can optimize the Ridge model by changing how much importance should the model should put on simplicity versus training set performance by using alpha parameter. The above ridge model uses the default alpha parameter = 1.0. The optimum setting of the alpha depends on particular dataset, but increasing alpha forces the weights to move toward zero, which decreases training set performance. You can test the later with different values of alpha passed to ridge object when instantiated e.g., `lr_ridge = Ridge(alpha=10)`.\n",
    "\n",
    "\n",
    "## Lasso \n",
    "\n",
    "Lasso is an alternative regularization approach for Ridge. Similar to Ridge, Lasso restricts the coefficients to be close to zero, but in a sightly different way called L1 Regularization. The consequence of L1 Regularization is that when using Lasso, some coefficients are exactly zero. This means some features are ignored entirely, hence some form of automatic feature selection. Having some coefficients exactly zero makes the model easier to interpret and can reveal some important features of the model. Apply lasso to the problem to observe any changes in performance. Similar to Ridge, you can adjust alpha parameter for Lasso model. This means controlling how strongly weights are pushed to zero. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Task 5 Model refinement with regularization (Lasso)\n",
    "\n",
    "## Import required libraries\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso = Lasso()\n",
    "# Train a Lasso model 'lasso' with training set\n",
    "# ===== YOUR CODE HERE ==========\n",
    "\n",
    "\n",
    "# Make predictions 'ridge_pred_train' on train set 'X_train' using the trained model and calculate the R^2\n",
    "# ===== YOUR CODE HERE ==========\n",
    "lasso_pred_train = 0 # modify\n",
    "R_square_train_lasso = 0 # modify\n",
    "\n",
    "print(\"R^2 score on trainig set: \", R_square_train_lasso)\n",
    "\n",
    "\n",
    "# Make predictions 'ridge_pred_test' on test set 'X_test' using the trained model and calculate the R^2\n",
    "# ===== YOUR CODE HERE ==========\n",
    "lasso_pred_test = 0 # modify\n",
    "R_square_test_lasso = 0 # modify\n",
    "\n",
    "print(\"R^2 score on trainig set: \", R_square_test_lasso)\n",
    "\n",
    "\n",
    "\n",
    "# Check the number of features the model is training with\n",
    "print(\"Number of features used(Lasso):\", np.sum(lasso.coef_!=0))\n",
    "\n",
    "\n",
    "\n",
    "# (Optional) Using Neptune, track the Lasso model performance.\n",
    "# ===== YOUR CODE HERE ==========\n",
    "\n",
    "\n",
    "# ================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How is the model performing on training set and test set with Lasso? Less scores indicate that you are underfitting and that it is using few number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print all scores\n",
    "all_scores = {'Regression':[R_square_train, R_square_test], \n",
    "              'Ridge':[R_square_train_ridge, R_square_test_ridge], \n",
    "              'Lasso':[R_square_train_lasso, R_square_test_lasso]}\n",
    "\n",
    "score_df = pd.DataFrame(data=all_scores)\n",
    "score_df\n",
    "\n",
    "\n",
    "# (Optional) Compare the output of 'score_df' with the score values tracked by Neptune via the its dashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, ridge regression is usually the first choice between two models. However, if you have a large amount of features and expect only a few of them to be important, Lasso might be a better choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting simple linear model is not ideal. You can work to further improve the model by engineering the feature (e.g., by training on few but high predictors) or employing regularization (e.g., ridge and lasso models). "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
